{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_and_std(metrics):\n",
    "    \"\"\"\n",
    "    Helper function to compute the average and standard deviation\n",
    "    for the list of metric values.\n",
    "    \"\"\"\n",
    "    avg = np.mean(metrics)\n",
    "    std = np.std(metrics)\n",
    "    return avg, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model_data(model_data):\n",
    "    \"\"\"\n",
    "    Process each model's data to calculate average and standard deviation\n",
    "    for each label and its metrics (accuracy, sensitivity, specificity, time).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for label, metrics in model_data.items():\n",
    "        accuracy_avg, accuracy_std = compute_avg_and_std(metrics['accuracy'])\n",
    "        sensitivity_avg, sensitivity_std = compute_avg_and_std(metrics['sensitivity'])\n",
    "        specificity_avg, specificity_std = compute_avg_and_std(metrics['specificity'])\n",
    "        time_avg, time_std = compute_avg_and_std(metrics['time'])\n",
    "\n",
    "        results[label] = {\n",
    "            'accuracy': {'average': accuracy_avg, 'std_dev': accuracy_std},\n",
    "            'sensitivity': {'average': sensitivity_avg, 'std_dev': sensitivity_std},\n",
    "            'specificity': {'average': specificity_avg, 'std_dev': specificity_std},\n",
    "            'time': {'average': time_avg, 'std_dev': time_std},\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_and_compute(json_file_path):\n",
    "    \"\"\"\n",
    "    Main function to read the JSON file and calculate the average and standard deviation\n",
    "    for all models and labels.\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    for model, model_data in data.items():\n",
    "        final_results[model] = process_model_data(model_data)\n",
    "    \n",
    "    return final_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage example\n",
    "# Replace 'your_json_file.json' with the actual path of your JSON file\n",
    "result = parse_and_compute('embedding_model_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_wise_table(data):\n",
    "    model_data = []\n",
    "    \n",
    "    for model_name, diseases in data.items():\n",
    "        for disease, metrics in diseases.items():\n",
    "            sensitivity = f\"{metrics['sensitivity']['average']} ± {metrics['sensitivity']['std_dev']}\"\n",
    "            specificity = f\"{metrics['specificity']['average']} ± {metrics['specificity']['std_dev']}\"\n",
    "            accuracy = f\"{metrics['accuracy']['average']} ± {metrics['accuracy']['std_dev']}\"\n",
    "            model_data.append([model_name, disease, sensitivity, specificity, accuracy])\n",
    "    \n",
    "    df_label_comparison = pd.DataFrame(model_data, columns=[\"Model\", \"Label\", \"Sensitivity\", \"Specificity\", \"Accuracy\"])\n",
    "    return df_label_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_model_table(data):\n",
    "    aggregated_data = {}\n",
    "    \n",
    "    for model_name, diseases in data.items():\n",
    "        accuracy_avg = 0\n",
    "        sensitivity_avg = 0\n",
    "        specificity_avg = 0\n",
    "        accuracy_std = 0\n",
    "        sensitivity_std = 0\n",
    "        specificity_std = 0\n",
    "        num_labels = len(diseases)\n",
    "        \n",
    "        for disease, metrics in diseases.items():\n",
    "            accuracy_avg += metrics['accuracy']['average']\n",
    "            sensitivity_avg += metrics['sensitivity']['average']\n",
    "            specificity_avg += metrics['specificity']['average']\n",
    "            \n",
    "            accuracy_std += metrics['accuracy']['std_dev']\n",
    "            sensitivity_std += metrics['sensitivity']['std_dev']\n",
    "            specificity_std += metrics['specificity']['std_dev']\n",
    "        \n",
    "        # Calculate mean and std_dev across all labels\n",
    "        accuracy_avg /= num_labels\n",
    "        sensitivity_avg /= num_labels\n",
    "        specificity_avg /= num_labels\n",
    "        \n",
    "        accuracy_std /= num_labels\n",
    "        sensitivity_std /= num_labels\n",
    "        specificity_std /= num_labels\n",
    "        \n",
    "        # Store result with ± formatting\n",
    "        aggregated_data[model_name] = {\n",
    "            \"average_accuracy\": f\"{accuracy_avg} ± {accuracy_std}\",\n",
    "            \"average_sensitivity\": f\"{sensitivity_avg} ± {sensitivity_std}\",\n",
    "            \"average_specificity\": f\"{specificity_avg} ± {specificity_std}\"\n",
    "        }\n",
    "    \n",
    "    df_aggregated_comparison = pd.DataFrame(aggregated_data).T\n",
    "    return df_aggregated_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables\n",
    "df_label_wise = create_label_wise_table(result)\n",
    "df_aggregated_model = create_aggregated_model_table(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_wise.to_csv('embedding_model_label_wise_comparison.csv', index=False, encoding='utf-8')\n",
    "df_aggregated_model.to_csv('embedding_model_aggregated_comparison.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
